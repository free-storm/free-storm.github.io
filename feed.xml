<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>自由风暴</title>
  <subtitle>自由风暴</subtitle>
  <id>http://freestorm.org/</id>
  <link href="http://freestorm.org/"/>
  <link href="http://freestorm.org/feed.xml" rel="self"/>
  <updated>2015-09-29T00:00:00+08:00</updated>
  <author>
    <name>自由风暴</name>
  </author>
  <entry>
    <title>暴走漫画 Docker 实践——Dockone 分享</title>
    <link rel="alternate" href="/2015/09/29/暴走漫画Docker实践.html"/>
    <id>/2015/09/29/暴走漫画Docker实践.html</id>
    <published>2015-09-29T00:00:00+08:00</published>
    <updated>2015-09-29T00:00:00+08:00</updated>
    <author>
      <name>Michael Ding</name>
    </author>
    <summary type="html">&lt;p&gt;大家好，我叫丁彦，来自暴走漫画。&lt;/p&gt;

&lt;p&gt;暴走漫画是一家文化传媒公司。公司除了有若干视频娱乐节目，还有相应的社区网站及 App。流量 UV 200w/天 左右，PV 千万。
为了更加有效地运营以及推荐用户个性化，2015年成立了数据部，负责暴漫的数据分析和数据挖掘相关服务。&lt;/p&gt;

&lt;p&gt;暴漫没有自己的服务器，是使用的国内某云服务。暴漫的后端主要是基于 Ruby 开发。也有基于 go, python 的一些micro service。
Docker 在暴漫中的应用主要包括：
* 开发环境的 service ...&lt;/p&gt;</summary>
    <content type="html">&lt;p&gt;大家好，我叫丁彦，来自暴走漫画。&lt;/p&gt;

&lt;p&gt;暴走漫画是一家文化传媒公司。公司除了有若干视频娱乐节目，还有相应的社区网站及 App。流量 UV 200w/天 左右，PV 千万。
为了更加有效地运营以及推荐用户个性化，2015年成立了数据部，负责暴漫的数据分析和数据挖掘相关服务。&lt;/p&gt;

&lt;p&gt;暴漫没有自己的服务器，是使用的国内某云服务。暴漫的后端主要是基于 Ruby 开发。也有基于 go, python 的一些micro service。
Docker 在暴漫中的应用主要包括：
* 开发环境的 service 搭建
* 代码托管，持续集成，docker 镜像，等若干 support 服务
* 部分 micro service 以及整个数据服务系统&lt;/p&gt;

&lt;p&gt;所以今天的内容是一些中小规模以及国内云服务下的 docker 实践的相关心得，主要包括在数据服务的架构及 docker 化的部署。&lt;/p&gt;

&lt;h2&gt;1. 简单介绍下开发环境以及 support 服务的 docker 应用&lt;/h2&gt;

&lt;p&gt;由于开发环境主要是 Mac，也有少量 Ubuntu 和 Windows，所以主要采用 Vagrant + docker 方式。
将 micro service 做成 image，在 Vagrant 中起相应的container，把端口暴露给 Host(Vagrant)。本地跑 Ruby(on Rails)&lt;/p&gt;

&lt;p&gt;support 服务的话，其他都很简单，只有持续集成介绍下。我们用的 gitlab ci。gitlab ci 支持将 task 跑在 docker container 里面
所以我们为不同的项目准备不同的测试环境(image)以及外部依赖(eg. mysql, redis)，然后在对应的 container 里面跑测试。
关于部署的话，我们平时的开发在 develop 分支，一旦向 master 分支合并后，会触发部署的 task。
部署的 task 跑在特定的 container 里面，这个 container 共享了 Host 的 docker unix sock 文件，可以执行 docker build, push 等命令&lt;/p&gt;

&lt;p&gt;关于开发环境和 support 服务的 docker 应用，因为不是今天的重点，并且前面也有很多朋友做过类似的介绍，所以先简单介绍到这里。&lt;/p&gt;

&lt;h2&gt;2. micro service 和 数据服务系统的 docker 应用&lt;/h2&gt;

&lt;p&gt;今年我们做了很多 micro service 的尝试，例如消息推送，推荐系统，反垃圾系统，数据分析系统，视频抓取等等若干子系统的拆分上线。
虽然过程是痛苦的，但是结果却是令人欣慰的。这些 micro service，几乎都是基于 docker 的。&lt;/p&gt;

&lt;h3&gt;2.1 Rails + docker 化的 micro service&lt;/h3&gt;

&lt;p&gt;整体来说，我们是个混合的架构，Rails 是正常的跑在云主机中的，micro service 跑在 docker 中。为了协调好各方，我们对基础服务做了一点小小的调整。&lt;/p&gt;

&lt;p&gt;这里不得不说说我做架构的一点心得。好的架构除了能满足业务需求，还要是与特定的团队，特定的资源所配套的。
在暴漫，由于技术力量有限，开发排期满，所以我都是尽量采用“非侵入式”的方案，这在后面的数据服务的构建中也有体现。&lt;/p&gt;

&lt;p&gt;首先，我们给所有的机器都装上了 docker
其次，我们搭建了一个 etcd 集群，将所有的云主机都纳入了 etcd 集群。而 etcd 也是跑在 docker 里的。
为了方便的跑起来 etcd，我们写了个一套 bash + python 的脚本(Python 的脚本也是跑在 docker 里的)
然后，所有的机器直接访问本机 IP 可以 access etcd。&lt;/p&gt;

&lt;p&gt;这里插一句，我们没有去折腾如何让docker跨主机组网，而是直接采用映射到 host的方式。一方面国内云主机只能这么干。另一方面，我们之前使用云主机也是单个主机特定用途的。
另外，在生产环境中，我们大量的使用了 shell + etcd 来启动 docker container 的方式。可以给大家看个 etcd 的启动 script。这个 script 放到最初的机器上就可以方便地启动起来etcd 集群。&lt;/p&gt;
&lt;pre&gt;&lt;code class="highlight shell"&gt;&lt;span class="c"&gt;#!/bin/bash&lt;/span&gt;

check_non_empty&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="c"&gt;# $1 is the content of the variable in quotes e.g. "$FROM_EMAIL"&lt;/span&gt;
  &lt;span class="c"&gt;# $2 is the error message&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; &lt;span class="nv"&gt;$1&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;""&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt;; &lt;span class="k"&gt;then
  &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;"ERROR: specify &lt;/span&gt;&lt;span class="nv"&gt;$2&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
  &lt;span class="nb"&gt;exit&lt;/span&gt; -1
  &lt;span class="k"&gt;fi&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

check_exec_success&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="c"&gt;# $1 is the content of the variable in quotes e.g. "$FROM_EMAIL"&lt;/span&gt;
  &lt;span class="c"&gt;# $2 is the error message&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; &lt;span class="nv"&gt;$1&lt;/span&gt; !&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"0"&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt;; &lt;span class="k"&gt;then
  &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;"ERROR: &lt;/span&gt;&lt;span class="nv"&gt;$2&lt;/span&gt;&lt;span class="s2"&gt; failed"&lt;/span&gt;
  &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$3&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;
  &lt;span class="nb"&gt;exit&lt;/span&gt; -1
  &lt;span class="k"&gt;fi&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

up&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;

  &lt;span class="c"&gt;# create ${EtcdData}&lt;/span&gt;
  mkdir -p &lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;EtcdData&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt;

  &lt;span class="c"&gt;# pull pycsa docker image&lt;/span&gt;
  docker pull private/pycsa:latest

  check_exec_success &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$?&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="s2"&gt;"pulling 'pycsa' image"&lt;/span&gt;

  &lt;span class="c"&gt;# pull etcd docker image&lt;/span&gt;
  docker pull quay.io/coreos/etcd:latest

  check_exec_success &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$?&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="s2"&gt;"pulling 'etcd' image"&lt;/span&gt;

  &lt;span class="c"&gt;# build cluster nodes list for `-initial-cluster`&lt;/span&gt;
  &lt;span class="nv"&gt;cwd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
  &lt;span class="nv"&gt;ClusterNodes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;docker run --rm &lt;span class="se"&gt;\&lt;/span&gt;
    -v &lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;cwd&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt;:/data &lt;span class="se"&gt;\&lt;/span&gt;
    private/pycsa:latest &lt;span class="se"&gt;\&lt;/span&gt;
    python up.py cluster-nodes &lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;1&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt; &lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;ETCD_NAME&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt; &lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;HostIP&lt;/span&gt;&lt;span class="k"&gt;})&lt;/span&gt;

    check_exec_success &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$?&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;ClusterNodes&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt;
    &lt;span class="s2"&gt;"-a"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;BaseCmd&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt; -initial-cluster &lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;ClusterNodes&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    -initial-cluster-state existing
    &lt;span class="p"&gt;;;&lt;/span&gt;
    &lt;span class="s2"&gt;""&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;BaseCmd&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt; -initial-cluster &lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;ClusterNodes&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt; &lt;span class="se"&gt;\&lt;/span&gt;
    -initial-cluster-token bzetcd-cluster -initial-cluster-state new
    &lt;span class="p"&gt;;;&lt;/span&gt;
    &lt;span class="k"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;"Usage: ./etcd.sh up [-a]"&lt;/span&gt;
    &lt;span class="nb"&gt;exit &lt;/span&gt;1
    &lt;span class="p"&gt;;;&lt;/span&gt;
    &lt;span class="k"&gt;esac&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;

  start&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    docker &lt;span class="nb"&gt;kill &lt;/span&gt;etcd 2&amp;gt;/dev/null
    docker rm etcd 2&amp;gt;/dev/null
    &lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;BaseCmd&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;}&lt;/span&gt;

  stop&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    docker stop etcd
    docker rm etcd
  &lt;span class="o"&gt;}&lt;/span&gt;


  &lt;span class="c"&gt;##################&lt;/span&gt;
  &lt;span class="c"&gt;# Start of script&lt;/span&gt;
  &lt;span class="c"&gt;##################&lt;/span&gt;

  &lt;span class="c"&gt;# source env&lt;/span&gt;
  . /etc/default/etcd

  check_non_empty &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;ETCD_NAME&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="s2"&gt;"ETCD_NAME"&lt;/span&gt;

  &lt;span class="c"&gt;# get host ip&lt;/span&gt;
  &lt;span class="nv"&gt;HostIP&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;ifconfig eth0 | awk &lt;span class="s1"&gt;'/\&amp;lt;inet\&amp;gt;/ { print $2}'&lt;/span&gt; | sed &lt;span class="s1"&gt;'s/addr://g'&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;

  &lt;span class="c"&gt;# set data dir&lt;/span&gt;
  &lt;span class="nv"&gt;EtcdData&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/data/etcd/data

  &lt;span class="c"&gt;# create etcd container base cmd&lt;/span&gt;
  &lt;span class="nv"&gt;BaseCmd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"docker run -d &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;&lt;span class="s2"&gt;
  -v /usr/share/ca-certificates/:/etc/ssl/certs &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;&lt;span class="s2"&gt;
  -v &lt;/span&gt;&lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;EtcdData&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;:/data &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;&lt;span class="s2"&gt;
  -p 4001:4001 -p 2380:2380 -p 2379:2379 &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;&lt;span class="s2"&gt;
  --name etcd quay.io/coreos/etcd:latest &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;&lt;span class="s2"&gt;
  -name &lt;/span&gt;&lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;ETCD_NAME&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;&lt;span class="s2"&gt;
  -data-dir /data &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;&lt;span class="s2"&gt;
  -advertise-client-urls http://&lt;/span&gt;&lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;HostIP&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;:2379,http://&lt;/span&gt;&lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;HostIP&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;:4001 &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;&lt;span class="s2"&gt;
  -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;&lt;span class="s2"&gt;
  -initial-advertise-peer-urls http://&lt;/span&gt;&lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;HostIP&lt;/span&gt;&lt;span class="k"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;:2380 &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;&lt;span class="s2"&gt;
  -listen-peer-urls http://0.0.0.0:2380"&lt;/span&gt;

  &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="k"&gt;in
  &lt;/span&gt;up&lt;span class="p"&gt;)&lt;/span&gt; up &lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="nv"&gt;$2&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt; &lt;span class="p"&gt;;;&lt;/span&gt;
  start&lt;span class="p"&gt;)&lt;/span&gt; start &lt;span class="p"&gt;;;&lt;/span&gt;
  stop&lt;span class="p"&gt;)&lt;/span&gt; stop &lt;span class="p"&gt;;;&lt;/span&gt;
  restart&lt;span class="p"&gt;)&lt;/span&gt;
  stop
  start
  &lt;span class="p"&gt;;;&lt;/span&gt;
  &lt;span class="k"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;"Usage: ./etcd.sh start|stop|restart or ./etcd.sh up [-a]"&lt;/span&gt;
  &lt;span class="nb"&gt;exit &lt;/span&gt;1
  &lt;span class="p"&gt;;;&lt;/span&gt;
  &lt;span class="k"&gt;esac&lt;/span&gt;

  &lt;span class="nb"&gt;exit &lt;/span&gt;0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;解释下， &lt;code&gt;up.py&lt;/code&gt; 是个 python 的脚本，跑在一个 pycsa 的容器里，这个容器有 python 环境以及相关的 package&lt;/p&gt;

&lt;p&gt;这样原来的服务几乎不受任何影响，我们可以利用 etcd + docker + shell script 来组建新的服务。&lt;/p&gt;

&lt;h3&gt;2.2 数据服务&lt;/h3&gt;

&lt;p&gt;我们的数据服务包括数据分析和数据挖掘两大块。数据分析主要是为了给运营提供量化的效果评估以及指导。数据挖掘则包括推荐，反垃圾等。&lt;/p&gt;

&lt;p&gt;数据服务的基础是数据流，即：数据收集-&amp;gt;数据分发-&amp;gt;数据处理&amp;lt;-&amp;gt;数据存储&lt;/p&gt;

&lt;p&gt;先给大家看个整体的架构图，由于本人不擅作图，所以直接用手画的，还请见谅。。&lt;/p&gt;

&lt;p&gt;&lt;img alt="fig" src="/images/baozou-data-arch-3a482905.png" /&gt;&lt;/p&gt;

&lt;p&gt;首先数据收集部分，就像之前说的，我尽量采用“非侵入式”的方案，所以，我们的整个数据收集都是基于日志的。
我们在每个应用服务器上装了 logstash (跑在 docker 中) 来收集各个应用服务器的日志，然后打到 kafka (跑在 docker 中) 里，给不同的用途使用。&lt;/p&gt;

&lt;p&gt;一份COPY 直接由kafka 一端的 logstash 存储到 elasticsearch(跑在 docker 中) 中
一份COPY 经过 spark (跑在 docker 中) stream 做实时处理(包括一些特定日志的提取)，然后将处理的结果存储在 elasticsearch 里
还有一份 COPY 直接存储到 HDFS (由云服务商提供)&lt;/p&gt;

&lt;p&gt;这里有个小问题，比如有些数据本身日志里并没有，比如用户的点击行为。这个时候，我们专门开发了一些 &amp;ldquo;ping&amp;rdquo; 接口，这些接口通过 Nginx 直接返回 200，并记录相关日志&lt;/p&gt;

&lt;p&gt;此外还有一部分数据，例如一些比较需要“较严格的完备”的，例如用于推荐系统，反垃圾系统学习的数据，我们存储在 SQL 数据库中&lt;/p&gt;

&lt;p&gt;下面我做些稍微详细的介绍&lt;/p&gt;

&lt;h4&gt;2.2.1 数据分析&lt;/h4&gt;

&lt;p&gt;数据分析有两种：实时数据分析和离线数据分析&lt;/p&gt;

&lt;p&gt;实时数据分析从 kafka 到 spark stream，处理结果进 elasticsearch，离线分析是定时任务，从 HDFS 到 spark，处理结果进 elasticsearch。一般来说，离线的结果会逐步包含实时的结果，
同时实时的结果领先于离线分析的结果。&lt;/p&gt;

&lt;p&gt;这里的分析有些抽象，我来举个例子：&lt;/p&gt;

&lt;p&gt;Q: 统计某个板块同时在线人数的变化趋势
A: 用户每次访问都有日志，日志里包括访问内容以及用户标识。首先 spark stream 从日志里抽取出特定板块不同用户的访问事件，以秒为单位合并相同用户事件。这就是分析结果：时间戳：人数&lt;/p&gt;

&lt;p&gt;然后这个结果怎么用？&lt;/p&gt;

&lt;p&gt;elasticsearch 有很强大的 agg 接口。你可以以1秒，10秒，1分等等各种时间间隔单位聚合这段时间内的在线人数，聚合方式用 &amp;lsquo;平均&amp;#39;或&amp;#39;最大&amp;rsquo;&lt;/p&gt;

&lt;h4&gt;2.2.2 数据挖掘&lt;/h4&gt;

&lt;p&gt;我们主要做了2个具体的数据挖掘系统：推荐+反垃圾&lt;/p&gt;

&lt;p&gt;今天主要讲下架构。&lt;/p&gt;

&lt;p&gt;这两个系统基本上步骤是一样的，分为2步：训练(train) 和 服务(serve)&lt;/p&gt;

&lt;p&gt;在 train 阶段，定时起一个 spark job，从训练数据集中读取数据，学习出 model，然后将 model 存储成文件
在 serve 阶段，起一个带 serve 的 spark job，load 之前学习出来的model 文件进内存，然后接受外部api 调用，返回结果。&lt;/p&gt;

&lt;p&gt;关于服务的开发这部分因为涉及到太多额外的知识，我就不多说了。&lt;/p&gt;

&lt;p&gt;这里讲个难点：spark 的 docker 化。&lt;/p&gt;

&lt;h4&gt;2.2.3 Spark 的 docker 化&lt;/h4&gt;

&lt;p&gt;Spark 的 docker 化分为两个部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;docker 化的 spark 集群&lt;/li&gt;
&lt;li&gt;docker 化的 spark 调用&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Spark 和我们一般用的服务不太一样，它的集群不是提供运算服务的，而是一种资源分配的调度器。
让 Spark 跑 Job，其实是起的一个 Spark 的本地程序，这个本地程序会向 cluster 要资源(其他机器)，cluster 分配资源以后，这个 spark 程序就把一些工作放在这些资源当中运行(进程)&lt;/p&gt;

&lt;p&gt;所以 Spark 的 docker 化分为两个部分。&lt;/p&gt;

&lt;p&gt;对于 spark 调用，也就是启动 spark 的本地程序，我们就是在跑程序的 image 中集成 java 环境，spark 程序&lt;/p&gt;

&lt;p&gt;对于 spark 集群，稍微复杂一些。spark 支持三种集群：mesos, yard，还有 spark 自己的一个 standalone
我们搭建的 spark standalone 集群，这还是考虑到我们自身的资源与需求。&lt;/p&gt;

&lt;p&gt;由于没找到官方的 spark docker image，我们自己做了一个，就是 java 环境 + spark 程序
然后利用 script + etcd 以不同的姿势(master 或 slave)在不同的云主机上启动 spark container&lt;/p&gt;

&lt;p&gt;官方推荐要起3个 master, 用 zookeeper 做 quorum，这个我们最近正在搞，还没上线，就不分享。我们现在线上跑的是 1 master  + 7 slave&lt;/p&gt;

&lt;p&gt;谢谢&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>冲突的处理 - 分布式数据库相关理论 Part5</title>
    <link rel="alternate" href="/2015/04/29/冲突的处理.html"/>
    <id>/2015/04/29/冲突的处理.html</id>
    <published>2015-04-29T00:00:00+08:00</published>
    <updated>2015-04-29T00:00:00+08:00</updated>
    <author>
      <name>Michael Ding</name>
    </author>
    <summary type="html">&lt;p&gt;冲突的处理，也是分布式系统中一个重要的议题。今天我们继续以 Riak 为案例，看看 Riak 是怎么做冲突处理的。&lt;/p&gt;

&lt;h2&gt;Vector Clock(向量钟)&lt;/h2&gt;

&lt;p&gt;Riak 通过一种叫做 &lt;code&gt;Vector Clock&lt;/code&gt; 的机制来处理冲突问题。简单来说，&lt;code&gt;Vector Clock&lt;/code&gt; 是一段 &lt;code&gt;token&lt;/code&gt;，
像 &lt;code&gt;Riak&lt;/code&gt; 这样的分布式系统通过这样的 &lt;code&gt;token&lt;/code&gt; 来追踪数据更新操作的先后顺序。&lt;/p&gt;

&lt;p&gt;在冲突处理中，能够知道冲突操作(eg. 创建操作，更改操作)的顺序，是非常重要的。
因为对于分布式系统来说，不同的客户端...&lt;/p&gt;</summary>
    <content type="html">&lt;p&gt;冲突的处理，也是分布式系统中一个重要的议题。今天我们继续以 Riak 为案例，看看 Riak 是怎么做冲突处理的。&lt;/p&gt;

&lt;h2&gt;Vector Clock(向量钟)&lt;/h2&gt;

&lt;p&gt;Riak 通过一种叫做 &lt;code&gt;Vector Clock&lt;/code&gt; 的机制来处理冲突问题。简单来说，&lt;code&gt;Vector Clock&lt;/code&gt; 是一段 &lt;code&gt;token&lt;/code&gt;，
像 &lt;code&gt;Riak&lt;/code&gt; 这样的分布式系统通过这样的 &lt;code&gt;token&lt;/code&gt; 来追踪数据更新操作的先后顺序。&lt;/p&gt;

&lt;p&gt;在冲突处理中，能够知道冲突操作(eg. 创建操作，更改操作)的顺序，是非常重要的。
因为对于分布式系统来说，不同的客户端连接到的是不同的服务器节点，
当一个客户端更新了一个服务器节点上的数据，也许另一个客户端也同时更新了另一个服务器节点上的数据。&lt;/p&gt;

&lt;p&gt;这时候，也许你会想到：记录每个操作的时间戳，然后依照时间戳靠后的操作来。然而要这么做的话，这里有个隐含的前提：
在这个分布式系统中的每个服务器节点，时钟都必须是完全同步的。
然而事实上，一方面这是非常困难的：需要非常大的财力物力的投入；另一方面，整个系统又是单点故障的。&lt;/p&gt;

&lt;p&gt;所以，Riak 使用 &lt;code&gt;Vector Clocks&lt;/code&gt; 来处理冲突。&lt;code&gt;Vector Clocks&lt;/code&gt; 给每个写操作(创建，更改，删除) 打上一个标签，标签代表了是哪个客户端以什么样的顺序执行的操作。
这样一来，客户端或者开发者就能决定面对冲突，该怎么决定。
如果你熟悉像 &lt;code&gt;Git&lt;/code&gt;, &lt;code&gt;Subversion&lt;/code&gt;这样的版本控制系统，
这就和两个人同时修改了同一个文件产生的冲突解决思路是相似的。&lt;/p&gt;

&lt;h3&gt;&lt;code&gt;Vector Clock&lt;/code&gt; 小故事 —— &lt;code&gt;Vector Clock&lt;/code&gt; 相关理论&lt;/h3&gt;

&lt;p&gt;暴走大事件的编辑部每周都要整理下一期里要播报的新闻段子。&lt;/p&gt;

&lt;p&gt;假设负责整理新闻段子有3个人：王尼玛(A), 张全蛋(B), 纸巾&amp;copy;。他们需要确定最终的新闻段子的列表。新闻段子的列表存储在分布式的服务器中。&lt;/p&gt;

&lt;p&gt;每个人用自己的终端连接数据库。这些终端都有着唯一的标识，用来构建 vector clock。下面就让我们模拟一下，vector clock 是如何工作的。&lt;/p&gt;

&lt;p&gt;首先，王尼玛用自己的终端更新了列表&lt;/p&gt;
&lt;pre&gt;&lt;code class="highlight plaintext"&gt;vclock: A[0]
value: ['news xx']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，张全蛋先下载了这个列表，然后更新了这个列表&lt;/p&gt;
&lt;pre&gt;&lt;code class="highlight plaintext"&gt;vclock: A[0], B[0]
value: ['news xx', 'news xyy']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;张全蛋更新的同时(王尼玛做更新之后)，纸巾同样的下载了已有的列表，做了更新。&lt;/p&gt;
&lt;pre&gt;&lt;code class="highlight plaintext"&gt;vclock: A[0], C[0]
value: ['news xx', 'news yyz']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二天，张全蛋复查列表，由于纸巾的更新操作并不是在他之后的(而是和他同时的)，
这时候就产生了一个冲突，需要处理。&lt;/p&gt;

&lt;p&gt;他拿到两个值：&lt;/p&gt;
&lt;pre&gt;&lt;code class="highlight plaintext"&gt;vclock: A[0], B[0]
value: ['news xx', 'news xyy']
--
vclock: A[0], C[0]
value: ['news xx', 'news yyz']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;他需要解决这个冲突：于是他选择合并这两个值：&lt;/p&gt;
&lt;pre&gt;&lt;code class="highlight plaintext"&gt;vclock: A[0], C[0], B[1]
value: ['news xx', 'news xyy', 'news yyz']
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样一来，任何人之后获取到的就是这个最新的合并后的值了。&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Riak 中的 CAP - 分布式数据库相关理论 Part4</title>
    <link rel="alternate" href="/2015/04/28/Riak中的CAP.html"/>
    <id>/2015/04/28/Riak中的CAP.html</id>
    <published>2015-04-28T00:00:00+08:00</published>
    <updated>2015-04-28T00:00:00+08:00</updated>
    <author>
      <name>Michael Ding</name>
    </author>
    <summary type="html">&lt;p&gt;和&lt;a href="2015/04/25/Riak%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A8%A1%E5%9E%8B.html"&gt;上一篇博文&lt;/a&gt;一样，这次我们依旧以 Riak 为案例，来分析 &lt;code&gt;CAP&lt;/code&gt; 理论在一个实际的分布式数据库中的作用。&lt;/p&gt;

&lt;p&gt;如果你还不熟悉 &lt;code&gt;CAP&lt;/code&gt;，可以参考我之前的两篇博客 &lt;a href="/2015/04/22/%E7%90%86%E8%A7%A3CAP%E7%90%86%E8%AE%BA.html"&gt;理解 CAP 理论&lt;/a&gt;, &lt;a href="/2015/04/23/Eventual-Consistency(%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7).html"&gt;最终一致性&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;这次我们来看看，在 Riak 这样的分布式key-value数据库中，&lt;code&gt;CAP&lt;/code&gt;理论是怎么起作用的。&lt;/p&gt;

&lt;h2&gt;Nodes/Writes/Reads&lt;/h2&gt;

&lt;p&gt;首先还是让我们来明确几个概念。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;N&lt;/strong&gt; odes&lt;/p&gt;

&lt;p&gt;需要"最终"包含正确的值的服务器节点总数(正确的冗余数据拷贝数)。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;W&lt;/strong&gt; rites&lt;/p&gt;

&lt;p&gt;每次写操作，我们需...&lt;/p&gt;</summary>
    <content type="html">&lt;p&gt;和&lt;a href="2015/04/25/Riak%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A8%A1%E5%9E%8B.html"&gt;上一篇博文&lt;/a&gt;一样，这次我们依旧以 Riak 为案例，来分析 &lt;code&gt;CAP&lt;/code&gt; 理论在一个实际的分布式数据库中的作用。&lt;/p&gt;

&lt;p&gt;如果你还不熟悉 &lt;code&gt;CAP&lt;/code&gt;，可以参考我之前的两篇博客 &lt;a href="/2015/04/22/%E7%90%86%E8%A7%A3CAP%E7%90%86%E8%AE%BA.html"&gt;理解 CAP 理论&lt;/a&gt;, &lt;a href="/2015/04/23/Eventual-Consistency(%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7).html"&gt;最终一致性&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;这次我们来看看，在 Riak 这样的分布式key-value数据库中，&lt;code&gt;CAP&lt;/code&gt;理论是怎么起作用的。&lt;/p&gt;

&lt;h2&gt;Nodes/Writes/Reads&lt;/h2&gt;

&lt;p&gt;首先还是让我们来明确几个概念。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;N&lt;/strong&gt; odes&lt;/p&gt;

&lt;p&gt;需要&amp;quot;最终&amp;quot;包含正确的值的服务器节点总数(正确的冗余数据拷贝数)。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;W&lt;/strong&gt; rites&lt;/p&gt;

&lt;p&gt;每次写操作，我们需要确保最少有多少节点被更新。也就是说，我们在执行写操作的时候，不需要等待 N 个节点都成功被写入，
而只需要 W 个节点成功写入，这次写操作就返回成功，而其他节点是在后台进行同步。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt; eads&lt;/p&gt;

&lt;p&gt;每次读操作，我们需要确保最少读到几份冗余数据。也就是说，我们在执行读操作的时候，需要读到 R 个节点的数据才算读成功，否则读取失败。&lt;/p&gt;

&lt;p&gt;为什么要这三个变量？其实这三个变量直接关系到了 Riak 的 CAP 特性。下面我们就来一一说明：&lt;/p&gt;

&lt;h2&gt;Eventual Consistency(W + R &amp;lt;= N)&lt;/h2&gt;

&lt;p&gt;如下图所示：假设我们的 N=3, 设置 W + R &amp;lt;= N(例如：R=2, W=1)。这样我们的系统可以相对保证读写性能。
因为写操作只需要一个节点写入就返回成功。&lt;/p&gt;

&lt;p&gt;&lt;img alt="fig1" src="/images/w+r&amp;lt;=n-e653dec0.png" /&gt;&lt;/p&gt;

&lt;p&gt;然而这里有机率发生这样的情况：就像图中所示，我写入的是node1(versionB)，然后进行了一次读操作。
恰好这时候新数据尚未同步到node2, node3，而读操作又是从node2，node3取的值。由于这两个节点的值都是 version A，
所以得到的值便是 version A。&lt;/p&gt;

&lt;p&gt;不过随着时间的推移，node1 中的 versionB 会被同步到 node2 以及 node3 中。
这时候，再有读操作，得到的值便是最新值(versionB)了。&lt;/p&gt;

&lt;p&gt;这就是所谓的 Eventual Consistency。整个系统有着较高的读写性能，但一致性有所牺牲。&lt;/p&gt;

&lt;p&gt;如果我们需要加强一致性，可以通过调整 W, R, N 来实现。&lt;/p&gt;

&lt;p&gt;接下来我们会讨论如何调整 W，R，N 的关系来平衡读写性能和一致性(即 A 和 C 的平衡)。&lt;/p&gt;

&lt;h2&gt;通过调节 W,R,N 的关系来调节一致性和读写性能的关系&lt;/h2&gt;

&lt;p&gt;一种极端做法(下图所示)，我们可以设 W=N, R=1。其实这就是关系型数据库的做法。
通过确保每次写操作时，所有相关节点都被成功写入，来确保一致性。这样可以保证一致性，但是牺牲了写操作的性能。&lt;/p&gt;

&lt;p&gt;&lt;img alt="fig2" src="/images/w=nr=1-03f259cf.png" /&gt;&lt;/p&gt;

&lt;p&gt;还有一种极端做法，我们可以设W=1, R=N。这样，无论你向哪个node写入了数据，都会被读到。
然后你读到的N个值也可能包含旧的值，只要有办法分辨出哪个是最新的值就可以了
(Riak 是用一直叫向量钟(Vector Clock)的技术来判断的，我们会在后面的博客中做介绍)
这样可以保证一致性，但是牺牲了读操作的性能。&lt;/p&gt;

&lt;p&gt;&lt;img alt="fig3" src="/images/w=1r=n-d16553e5.png" /&gt;&lt;/p&gt;

&lt;p&gt;最后再给出一种被称作 &lt;code&gt;quorum&lt;/code&gt; 的做法。如下图所示，可以设置 W + R &amp;gt; N (例如 W=2, R=2)。这样同样可以保证一致性。
然而性能的损失由写操作和读操作共同承担。这种做法叫做 &lt;code&gt;quorum&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img alt="fig4" src="/images/w+r&amp;gt;n-a4cdd279.png" /&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Riak的分布式数据库模型 - 分布式数据库相关理论 Part3</title>
    <link rel="alternate" href="/2015/04/25/Riak的分布式数据库模型.html"/>
    <id>/2015/04/25/Riak的分布式数据库模型.html</id>
    <published>2015-04-25T00:00:00+08:00</published>
    <updated>2015-04-25T00:00:00+08:00</updated>
    <author>
      <name>Michael Ding</name>
    </author>
    <summary type="html">&lt;h2&gt;Riak 是什么&lt;/h2&gt;

&lt;p&gt;Riak 是一个 erlang 开发的开源的分布式 key-value 数据库，
在 &lt;code&gt;High Availability&lt;/code&gt;, &lt;code&gt;Fault Tolerance&lt;/code&gt;, &lt;code&gt;Scalability&lt;/code&gt; 方面表现优异。
其实现受  &lt;a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf"&gt;Amazon Dynamodb&lt;/a&gt; 启发，是一个很有代表性的分布式数据库。&lt;/p&gt;

&lt;p&gt;Riak 集群是一个去中心化的集群。每个服务器节点都是平等的，可以自由地添加和删除。
这使得 Riak 的故障转移(Failure Over)和扩展非常容易。
在 CAP 理论方面，Riak...&lt;/p&gt;</summary>
    <content type="html">&lt;h2&gt;Riak 是什么&lt;/h2&gt;

&lt;p&gt;Riak 是一个 erlang 开发的开源的分布式 key-value 数据库，
在 &lt;code&gt;High Availability&lt;/code&gt;, &lt;code&gt;Fault Tolerance&lt;/code&gt;, &lt;code&gt;Scalability&lt;/code&gt; 方面表现优异。
其实现受  &lt;a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf"&gt;Amazon Dynamodb&lt;/a&gt; 启发，是一个很有代表性的分布式数据库。&lt;/p&gt;

&lt;p&gt;Riak 集群是一个去中心化的集群。每个服务器节点都是平等的，可以自由地添加和删除。
这使得 Riak 的故障转移(Failure Over)和扩展非常容易。
在 CAP 理论方面，Riak 可以自由地在 CP 和 AP 之间做平衡。&lt;/p&gt;

&lt;h2&gt;理解 Riak 的分布式数据库模型&lt;/h2&gt;

&lt;h3&gt;Riak 的数据冗余&lt;/h3&gt;

&lt;p&gt;下面还是让我们从简单的例子开始，来理解下 Riak 的分布式数据库模型，包括数据的存储，节点服务器的，CAP理论的关系等。&lt;/p&gt;

&lt;p&gt;首先让我们先定义一个概念：&lt;code&gt;N&lt;/code&gt;，表示数据的&amp;quot;份数&amp;ldquo;。在分布式数据库中，一份数据往往会存储多份拷贝(所谓冗余，或者 replications)&lt;/p&gt;

&lt;p&gt;现在，假设我们有一个服务器节点(node1)，存有三个数据(key分别是 P0, P1, P2)，N = 1。那么可以想象，这三个数据都是存放在 node1 中。如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img alt="fig1" src="/images/1-node-with-3-data-e7c30ed4.png" /&gt;&lt;/p&gt;

&lt;p&gt;当 N = 2 时，假设 P0, P1, P2 的冗余数据分别是 R0, R1, R2， 那么可以想象，这6个数据也应该都存储在 node1 中，如 下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img alt="fig2" src="/images/1-node-with-3x2-data-90719dca.png" /&gt;&lt;/p&gt;

&lt;p&gt;这时候，让我们把服务器节点增加到2个(node1, node2)，那么可以想象，6个数据有很多中组合方式，例如下面这两种：&lt;/p&gt;

&lt;p&gt;&lt;img alt="fig3" src="/images/2-node-with-3x2-data-1-3864a6bb.png" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img alt="fig4" src="/images/2-node-with-3x2-data-2-758f4a81.png" /&gt;&lt;/p&gt;

&lt;p&gt;也许你发现了，他们有个共同点：&lt;strong&gt;同一个数据的冗余数据放在不同的服务器节点中&lt;/strong&gt;。这样就算一个节点删除(当机)了，集群的数据仍然能保证完整性。
这为故障转移(Failure over)提供了基础。&lt;/p&gt;

&lt;p&gt;那么现在的问题来了，&lt;strong&gt;是否有什么科学(公式化)的方式来找到分配这些数据的组合(之一)呢&lt;/strong&gt;？&lt;/p&gt;

&lt;h3&gt;Riak Ring&lt;/h3&gt;

&lt;p&gt;Riak 通过被称作 &lt;code&gt;Riak Ring&lt;/code&gt; 的东西来解决这个问题。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;首先&lt;/strong&gt;，Riak 将所有的 key 通过 hash 函数映射到一个 160 bit 的整数空间中。
即一个 key 对应着一个 0 ~ 2^160 - 1 的整数。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;然后&lt;/strong&gt;，Riak 引入了 vnode(虚拟节点) 的概念，vnode 个数是可以配置的，默认是 64。
160 bit 的整数会均匀的分布到所有的 vnode。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最后&lt;/strong&gt;，这些 vnode 会&amp;quot;均匀地&amp;quot;分配到 物理节点上。具体的分配的方法很巧妙，通过 &lt;code&gt;Riak Ring&lt;/code&gt; 这样的东西。&lt;/p&gt;

&lt;p&gt;下面我们用一幅图来具体解释下 &lt;code&gt;Riak Ring&lt;/code&gt;。图中，假设 vnode 32 个，服务器节点 4个。&lt;/p&gt;

&lt;p&gt;&lt;img alt="fig5" src="/images/riak-ring-99006345.png" /&gt;&lt;/p&gt;

&lt;p&gt;让我们把 160 bit 想像成一个环，环上的一小段代表一个 vnode。四种颜色分别代表 4 个服务器节点。&lt;/p&gt;

&lt;p&gt;2^160 个整数按照从小到大的顺序均匀地分布到 32 个 vnode 中，例如 2^159 是第 17 个 vnode 上的第一个整数。&lt;/p&gt;

&lt;p&gt;32 个 vnode 按照从小到大的顺序依次被分配到 4 个服务器节点上。即：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1, 5, 9&amp;hellip;29 vnode 分配给第1个服务器节点(node1)&lt;/li&gt;
&lt;li&gt;2, 6, 10&amp;hellip;30 vnode 分配给第1个服务器节点(node2)&lt;/li&gt;
&lt;li&gt;3, 7, 11&amp;hellip;31 vnode 分配给第1个服务器节点(node3)&lt;/li&gt;
&lt;li&gt;4, 8, 12&amp;hellip;32 vnode 分配给第1个服务器节点(node4)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;现在还剩下一个问题：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;冗余数据的存储&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们先假设 N = 3(即有2份冗余存储)&lt;/p&gt;

&lt;p&gt;假设要存储的数据，key 为 &lt;code&gt;test-key&lt;/code&gt; ，根据 Riak Ring 算出来，应该存储在 vnode6(即：node2)上。
那么 拷贝1 存储在 vnode7(即：node3)上，拷贝2 存储在 vnode8(即：node4)上。&lt;/p&gt;

&lt;p&gt;所以 Riak 对于冗余数据的存储策略是：&lt;strong&gt;将冗余数据依次存到下一个vnode中&lt;/strong&gt;。&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Eventual Consistency(最终一致性) - 分布式数据库相关理论 Part2</title>
    <link rel="alternate" href="/2015/04/23/Eventual-Consistency(最终一致性).html"/>
    <id>/2015/04/23/Eventual-Consistency(最终一致性).html</id>
    <published>2015-04-23T00:00:00+08:00</published>
    <updated>2015-04-23T00:00:00+08:00</updated>
    <author>
      <name>Michael Ding</name>
    </author>
    <summary type="html">&lt;h2&gt;1. Eventual Consistency 概述&lt;/h2&gt;

&lt;p&gt;分布式数据库必须要有 &lt;code&gt;分区容忍性(Partition Tolerant)&lt;/code&gt;，所以主要是在 &lt;code&gt;一致性(Consistent)&lt;/code&gt; 和 &lt;code&gt;可用性(Available)&lt;/code&gt; 之间做选择。
虽然在 CAP 理论中，选择了 &lt;code&gt;Availability&lt;/code&gt; 就不可能得到真正的 &lt;code&gt;Consistency&lt;/code&gt;，但是你可以追求 &lt;code&gt;最终一致性(Evental Consistency)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;evental Consistency&lt;/code&gt; 背后的思路是：每个系统节点总是 &lt;code&gt;Available&lt;/code&gt; 的，...&lt;/p&gt;</summary>
    <content type="html">&lt;h2&gt;1. Eventual Consistency 概述&lt;/h2&gt;

&lt;p&gt;分布式数据库必须要有 &lt;code&gt;分区容忍性(Partition Tolerant)&lt;/code&gt;，所以主要是在 &lt;code&gt;一致性(Consistent)&lt;/code&gt; 和 &lt;code&gt;可用性(Available)&lt;/code&gt; 之间做选择。
虽然在 CAP 理论中，选择了 &lt;code&gt;Availability&lt;/code&gt; 就不可能得到真正的 &lt;code&gt;Consistency&lt;/code&gt;，但是你可以追求 &lt;code&gt;最终一致性(Evental Consistency)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;evental Consistency&lt;/code&gt; 背后的思路是：每个系统节点总是 &lt;code&gt;Available&lt;/code&gt; 的，同时任何的写(修改数据)操作都会在后台同步给系统的其他节点。
这意味着，在任意时刻，整个系统是&lt;code&gt;Inconsistent(不一致的)&lt;/code&gt;，然而从概率上讲，大多数的请求得到的值是准确的。&lt;/p&gt;

&lt;p&gt;互联网的 DNS(域名服务) 就是最终一致性的一个非常好的例子。你注册了一个域名，
这个新域名需要几天的时间才能通知给所有的 DNS 服务器。但是不管什么时候，你能够连接到的任意 DNS 服务器对你来说都是 &amp;lsquo;Available&amp;rsquo; 的。&lt;/p&gt;

&lt;h2&gt;2. Eventual Consistency 小故事&lt;/h2&gt;

&lt;p&gt;让我们接着之前的&lt;a href="/2015/04/22/理解CAP理论.html"&gt;小故事&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;假设你不是深山里，是被抓到一个孤岛上造方舟。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2015年7月3日&lt;/code&gt;——距离你被抓来造方舟已经将近3个月，你在孤岛的海边捡到了一个漂流瓶，里面写着：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;最新一期暴走大事件是第四季第2期&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所以你知道：最新一期暴走大事件至少是&lt;code&gt;第四季第2期&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;假设暴漫的粉丝喜欢玩漂流瓶——只要暴走大事件有更新，就会把最新一期的暴走大事件写在纸上，然后通过漂流瓶扔向大海。
这样，像你这样的被 &lt;code&gt;Partition&lt;/code&gt; 的人，总是能时不时地收到记录着最新一期暴走大事件是什么的漂流瓶。
换句话说，虽然每一时刻，关于“最新一期暴走大事件是什么”你并不一定知道的是正确的答案，但你总是会 &lt;code&gt;eventually(最终)&lt;/code&gt; 知道正确答案。&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>理解 CAP 理论  - 分布式数据库相关理论 Part1</title>
    <link rel="alternate" href="/2015/04/22/理解CAP理论.html"/>
    <id>/2015/04/22/理解CAP理论.html</id>
    <published>2015-04-22T00:00:00+08:00</published>
    <updated>2015-04-22T00:00:00+08:00</updated>
    <author>
      <name>Michael Ding</name>
    </author>
    <summary type="html">&lt;p&gt;&lt;strong&gt;CAP&lt;/strong&gt; 是分布式数据库中的重要理论之一。为了更好的理解分布式数据库，我们需要对 CAP 理论有个简单的理解。&lt;/p&gt;

&lt;h2&gt;1.CAP 概述&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;CAP&lt;/strong&gt; 证明了，对于一个分布式数据库系统，存在这样三个指标：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;C&lt;/strong&gt;&lt;em&gt;onsistent&lt;/em&gt;(一致性。写操作是 &lt;code&gt;原子&lt;/code&gt; 的，当写操作完成后，所有后续的读取操作获取得到的都必须是新值),&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;A&lt;/strong&gt;&lt;em&gt;vailable&lt;/em&gt;(可用性。只要还有一个节点服务器在运行，整个系统对于请求总是要返回结果)&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;P&lt;/strong&gt;&lt;em&gt;artition tolerant&lt;/em&gt;(分区容忍性。当节点服务器之间的通信中断后，即：出现网络...&lt;/li&gt;
&lt;/ul&gt;</summary>
    <content type="html">&lt;p&gt;&lt;strong&gt;CAP&lt;/strong&gt; 是分布式数据库中的重要理论之一。为了更好的理解分布式数据库，我们需要对 CAP 理论有个简单的理解。&lt;/p&gt;

&lt;h2&gt;1.CAP 概述&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;CAP&lt;/strong&gt; 证明了，对于一个分布式数据库系统，存在这样三个指标：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;C&lt;/strong&gt;&lt;em&gt;onsistent&lt;/em&gt;(一致性。写操作是 &lt;code&gt;原子&lt;/code&gt; 的，当写操作完成后，所有后续的读取操作获取得到的都必须是新值),&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;&lt;em&gt;vailable&lt;/em&gt;(可用性。只要还有一个节点服务器在运行，整个系统对于请求总是要返回结果)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;P&lt;/strong&gt;&lt;em&gt;artition tolerant&lt;/em&gt;(分区容忍性。当节点服务器之间的通信中断后，即：出现网络分区，整个系统还是能提供服务的)。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而你只能在这三个指标中同时照顾好两个。&lt;/p&gt;

&lt;p&gt;根据 CAP 理论，当你在设计/使用分布式数据库时，你需要做出选择:在 Consistent, Available, Partition tolerant 中放弃什么。&lt;/p&gt;

&lt;p&gt;Partition tolerant 是个架构选择(数据库是否是分布式)，所以一般而言，你需要选择是更在意 Consistent 还是 Available。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;理解 CAP 理论对于做出正确的选择是至关重要的。&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;2.CAP 小故事&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;为了更好地理解 CAP，这里以现实生活中的例子做个类比&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;假设这个世界是一个巨大的分布式系统，关于暴走漫画的知识是系统中存储的数据，暴漫的粉丝是这个分布式系统中的一个个节点。&lt;/p&gt;

&lt;p&gt;假设今天你刚刚看了最新一期暴走大事件(第三季43集)，而今天的日期是 &lt;code&gt;2015年4月18日&lt;/code&gt;，突然有一伙儿神秘人闯进你家门，把你抓到了深山里，让你参与建造方舟，并且与世隔绝。&lt;/p&gt;

&lt;p&gt;时光如梭，一转眼 5 年过去了，到了 &lt;code&gt;2020年1月2日&lt;/code&gt;。方舟建成，你被送回了家乡。在回家的路上，你遇到一个路人，问了你一个问题：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;暴走大事件最新一期是第几季第几集了？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这时候，你需要做一个选择：&lt;/p&gt;

&lt;p&gt;你可以回答你知道的最新一期(第三季43集，5年前的最新一期)。如果你选择回答，那你就是 &lt;code&gt;Available&lt;/code&gt; 的。或者你可以选择不回答，因为你已经与世隔绝了 5  年，你知道你的答案很可能和世界上其余暴漫粉丝的答案不一致(&lt;code&gt;Consistent&lt;/code&gt;)，这样这个路人得不到答案，但是整个世界是 &lt;code&gt;Consistent&lt;/code&gt; 的。&lt;/p&gt;

&lt;p&gt;即：你可以选择确保路人能得到答案(Available)，或者确保世界的一致性(Consistent)。&lt;/p&gt;
</content>
  </entry>
</feed>
